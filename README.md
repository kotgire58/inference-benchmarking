---
title: "Inference Benchmarking Suite"
emoji: "âš¡"
colorFrom: "blue"
colorTo: "purple"
sdk: "streamlit"
sdk_version: "1.35.0"
app_file: "streamlit_app/Intro.py"
pinned: false
---


ğŸš€ Inference Benchmarking Suite (Streamlit Edition)

A modular, educational benchmarking toolkit designed to teach and demonstrate modern LLM inference optimizations â€” including batching, KV-cache reuse, speculative decoding, vLLM acceleration, and real-time chatbot comparisons â€” with a clean Streamlit UI.

This project is structured for hands-on exploration, making it ideal for learning how different inference strategies impact:

âš¡ Latency

ğŸ”¥ Throughput (tokens/sec)

ğŸ’¾ Memory usage

ğŸ’µ Cost efficiency

ğŸ¤– Real-time user experience

ğŸ“ Project Structure
INFERENCE-BENCHMARKING/
â”‚
â”œâ”€â”€ benchmarks/                 # Core benchmarking utilities
â”‚â”€â”€ models/                     # Model loading & backend wrappers
â”‚â”€â”€ optimizations/              # Optional custom optimization modules
â”‚
â”œâ”€â”€ streamlit_app/              # Main Streamlit application
â”‚   â”œâ”€â”€ Intro.py                # Home page (entry screen)
â”‚   â”œâ”€â”€ config.toml             # Streamlit multipage configuration
â”‚   â””â”€â”€ pages/
â”‚       â”œâ”€â”€ Batching.py         # Batching vs non-batching demo
â”‚       â”œâ”€â”€ ChatbotDemo.py      # Chat inference comparison
â”‚       â”œâ”€â”€ FinalBenchmark.py   # Unified benchmark runner
â”‚       â”œâ”€â”€ kv_cache.py         # KV-cache speedup visualization
â”‚       â”œâ”€â”€ SpeculativeDecoding.py # Draft model vs target model
â”‚       â””â”€â”€ VLLM.py             # vLLM-specific benchmark page
â”‚
â”œâ”€â”€ utils/                      # Shared helpers for timing, logging, etc.
â”‚
â”œâ”€â”€ run_benchmark.py            # CLI runner for benchmarking
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md                   # (this file)

ğŸ¯ What This App Teaches

Each page inside the Streamlit UI focuses on one inference concept, showing both code and performance impact:

ğŸ§© 1. Batching

Demonstrates how batching multiple prompts drastically increases throughput.

Shows tokens/sec vs batch size.

ğŸ’¡ 2. KV-Cache

Visualizes how reusing cached key/value tensors reduces decoding cost.

Demonstrates "streaming-like" speedup.

âš¡ 3. Speculative Decoding

Draft model generates N tokens â†’ target model verifies.

Shows latency reduction %.

ğŸ”¥ 4. vLLM Engine

Compares vanilla inference vs paged attention.

Great for understanding GPU memory efficiency.

ğŸ¤– 5. Chatbot Demo

Side-by-side inference comparison.

Helps visualize real-time responsiveness differences.

ğŸ“Š 6. Final Benchmark

A clean, unified benchmark runner measuring:

Latency

Throughput

Cost estimates

Stability across multiple runs

â–¶ï¸ Running Locally (Recommended for Testing)
1. Install dependencies
pip install -r requirements.txt

2. Launch the Streamlit app
streamlit run streamlit_app/Intro.py


You should now see the multipage UI load with all benchmark pages.